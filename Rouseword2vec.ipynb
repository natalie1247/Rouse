{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmAyzFyjt123TwKD3vxyEP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Rouse word2vec\n","\n","This code takes an input text file of a book (copy pasted from PDF if required), removes accents for Spanish and splits lines. This also contains slightly adapted code from https://www.tensorflow.org/tutorials/text/word2vec\n","which does the word2vec processing to produce vector files"],"metadata":{"id":"GJZst59SacEP"}},{"cell_type":"code","source":["# this cell is my own\n","\n","!pip install unidecode\n","from unidecode import unidecode\n","\n","book_file = input(\"Which book file do you want to read?\")\n","create_file = input(\"what do you want to call the new file?\")\n","\n","with open(book_file,\"r\") as f:\n","  new_file = open(create_file, \"w\")\n","  for line in f:\n","    line = unidecode(line) # removes Spanish accents\n","    if len(line) > 200:\n","      wordlist = line.split(\" \")   # splits lines if too long for processing\n","      while len(wordlist) > 15:\n","        new_line = []\n","        for count in range(0,15):\n","          word = wordlist[0]\n","          wordlist.remove(word)\n","          if not word.isdigit():   # removing numbers\n","            new_line.append(word)\n","            new_line.append(\" \")\n","          \n","        string_line = \"\".join([i for i in new_line])\n","        new_file.write(string_line)\n","        new_file.write(\"\\n\")\n","\n","      if len(wordlist) != 0:    # adding on last few words\n","        string_line = \"\" \n","        for word in wordlist:\n","          string_line.join(word)\n","          string_line.join(\" \")\n","        new_file.write(string_line)\n","        new_file.write(\"\\n\")\n","\n","\n","      else:  # if normal length line\n","        new_line = \"\".join([i for i in line if not i.isdigit()])\n","        new_file.write(new_line)\n","        new_file.write(\"\\n\")\n","\n","    else:\n","      new_file.write(line)"],"metadata":{"id":"R5F1UCDYjY5l"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ao5A3HSAA1bA"},"outputs":[],"source":["# This code is mostly not my own. It was originally written by the TensorFlow authors (https://www.tensorflow.org/tutorials/text/word2vec)\n","# I did not include all of the linked code (some is a demo) and changed lines to link to my file, and adjusted the number of epochs and neagtive samples\n","\n","import io\n","import re\n","import string\n","import tqdm\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","\n","# Load the TensorBoard notebook extension\n","%load_ext tensorboard\n","\n","SEED = 42\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n","  # Elements of each training example are appended to these lists.\n","  targets, contexts, labels = [], [], []\n","\n","  # Build the sampling table for `vocab_size` tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","  # Iterate over all sequences (sentences) in the dataset.\n","  for sequence in tqdm.tqdm(sequences):\n","\n","    # Generate positive skip-gram pairs for a sequence (sentence).\n","    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence,\n","          vocabulary_size=vocab_size,\n","          sampling_table=sampling_table,\n","          window_size=window_size,\n","          negative_samples=0)\n","\n","    # Iterate over each positive skip-gram pair to produce training examples\n","    # with a positive context word and negative samples.\n","    for target_word, context_word in positive_skip_grams:\n","      context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1,\n","          num_sampled=num_ns,\n","          unique=True,\n","          range_max=vocab_size,\n","          seed=seed,\n","          name=\"negative_sampling\")\n","\n","      # Build context and label vectors (for one target word)\n","      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n","      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","      targets.append(target_word)\n","      contexts.append(context)\n","      labels.append(label)\n","\n","  return targets, contexts, labels\n","\n","\n","\n","\n","path_to_file = input(\"Which file do you want to use?\") # I changed this line to my own file\n","\n","text_ds = tf.data.TextLineDataset(path_to_file)#.filter(lambda x: tf.cast(tf.strings.length(x), bool)) \n","print(text_ds)\n","\n","# Now, create a custom standardization function to lowercase the text and\n","# remove punctuation.\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  return tf.strings.regex_replace(lowercase,\n","                                  '[%s]' % re.escape(string.punctuation), '')\n","\n","\n","# Define the vocabulary size and the number of words in a sequence.\n","vocab_size = 4096\n","sequence_length = 10\n","\n","# Use the `TextVectorization` layer to normalize, split, and map strings to\n","# integers. Set the `output_sequence_length` length to pad all samples to the\n","# same length.\n","vectorize_layer = layers.TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)\n","\n","vectorize_layer.adapt(text_ds.batch(1024))\n","\n","# Save the created vocabulary for reference.\n","inverse_vocab = vectorize_layer.get_vocabulary()\n","print(inverse_vocab[:20])\n","\n","# Vectorize the data in text_ds.\n","text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()\n","\n","sequences = list(text_vector_ds.as_numpy_iterator())\n","print(len(sequences))\n","\n","for seq in sequences[:5]:\n","  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")\n","\n","\n","targets, contexts, labels = generate_training_data(\n","    sequences=sequences,\n","    window_size=2,\n","    num_ns=5,\n","    vocab_size=vocab_size,\n","    seed=SEED)\n","\n","targets = np.array(targets)\n","contexts = np.array(contexts)\n","labels = np.array(labels)\n","\n","print('\\n')\n","print(f\"targets.shape: {targets.shape}\")\n","print(f\"contexts.shape: {contexts.shape}\")\n","print(f\"labels.shape: {labels.shape}\")\n","\n","BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)\n","\n","dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","print(dataset)\n","\n","num_ns = 5\n","\n","class Word2Vec(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(Word2Vec, self).__init__()\n","    self.target_embedding = layers.Embedding(vocab_size,\n","                                      embedding_dim,\n","                                      input_length=1,\n","                                      name=\"w2v_embedding\")\n","    self.context_embedding = layers.Embedding(vocab_size,\n","                                       embedding_dim,\n","                                       input_length=num_ns+1)\n","\n","  def call(self, pair):\n","    target, context = pair\n","    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n","    # context: (batch, context)\n","    if len(target.shape) == 2:\n","      target = tf.squeeze(target, axis=1)\n","    # target: (batch,)\n","    word_emb = self.target_embedding(target)\n","    # word_emb: (batch, embed)\n","    context_emb = self.context_embedding(context)\n","    # context_emb: (batch, context, embed)\n","    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n","    # dots: (batch, context)\n","    return dots\n","\n","\n","def custom_loss(x_logit, y_true):\n","      return tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=y_true)\n","\n","embedding_dim = 128\n","word2vec = Word2Vec(vocab_size, embedding_dim)\n","word2vec.compile(optimizer='adam',\n","                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                 metrics=['accuracy'])\n","\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")\n","word2vec.fit(dataset, epochs=30, callbacks=[tensorboard_callback])\n","#docs_infra: no_execute\n","%tensorboard --logdir logs\n","\n","weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()\n","\n","out_v = io.open('h_vect10.tsv', 'w', encoding='utf-8')\n","out_m = io.open('h_meta10.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()\n","\n","\n","try:\n","  from google.colab import files\n","  files.download('h_vect10.tsv')\n","  files.download('h_meta10.tsv')\n","except Exception:\n","  pass"]}]}
